{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import expit, logit\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Tshirt/top','Trouser','Pull','Dress','Coat','Sandal',\n",
    "               'Shirt','Sneaker','Bag','Ankle boot']\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float64')\n",
    "X_test = X_test.astype('float64')\n",
    "y_train = y_train.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "img_rows, img_cols = 28, 28\n",
    "X_train = X_train/255.0\n",
    "X_train = X_train - np.mean(X_train, axis = 0)\n",
    "X_test = X_test/255.0\n",
    "X_test = X_test - np.mean(X_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_test = tf.convert_to_tensor(y_test)\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "num_classes = 10\n",
    "num_features = img_rows*img_cols\n",
    "numIter = 250000\n",
    "input_shape = [img_rows,img_cols,1]\n",
    "num_layers = np.array([4096,4096,num_classes])\n",
    "num_channel = np.array([96,256,384,384,256])\n",
    "kernel_size = np.array([11,5,3,3,3])\n",
    "sple_size = len(y_train)\n",
    "rho_0 = 10000\n",
    "rho_1 = 1\n",
    "u_0 = 0.5\n",
    "gamma = np.array(1e-7)\n",
    "minibatch_size = 100\n",
    "flatten = 6400\n",
    "sigmasq = 1.0\n",
    "conv_layer = 5\n",
    "pool_layer = 3\n",
    "full_layer = 3\n",
    "'''\n",
    "num_classes = 10\n",
    "num_features = img_rows*img_cols\n",
    "numIter = 10000\n",
    "input_shape = [img_rows,img_cols,1]\n",
    "num_layers = np.array([300,200,num_classes])\n",
    "num_channel = np.array([6,16])\n",
    "kernel_size = np.array([3,3])\n",
    "sple_size = len(y_train)\n",
    "rho_0 = 10000\n",
    "rho_1 = 1\n",
    "u_0 = 50\n",
    "flatten = 784\n",
    "gamma = np.array(1e-6)\n",
    "minibatch_size = 100\n",
    "sigmasq = 1.0\n",
    "conv_layer = 2\n",
    "pool_layer = 2\n",
    "full_layer = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(filters=96, kernel_size=11, strides=4,\n",
    "                               activation='relu',input_shape= input_shape,name = 'Conv1'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2))\n",
    "model.add(keras.layers.Conv2D(filters=256, kernel_size=5, padding='same',\n",
    "                               activation='relu',name = 'Conv2'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2))\n",
    "model.add(keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',\n",
    "                               activation='relu',name = 'Conv3'))\n",
    "model.add(keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',\n",
    "                               activation='relu',name = 'Conv4'))\n",
    "model.add(keras.layers.Conv2D(filters=256, kernel_size=3, padding='same',\n",
    "                               activation='relu',name = 'Conv5'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(4096, activation='relu',name = 'dense1'))\n",
    "model.add(keras.layers.Dense(4096, activation='relu',name = 'dense2'))\n",
    "model.add(keras.layers.Dense(10,activation = \"softmax\",name = 'dense3'))\n",
    "'''\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(num_channel[0], (3, 3), activation='relu',input_shape= input_shape ,padding = 'same',strides = 1))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(num_channel[1], (3, 3), activation='relu', padding = 'same',strides = 1))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(num_layers[0],activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(num_layers[1],activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(num_layers[2],activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298650, 647.5635776921711, 60000.0)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0\n",
    "pre_channel = 1\n",
    "for i in range(len(num_channel)):\n",
    "    p += pre_channel*kernel_size[i]**2*num_channel[i] + num_channel[i]\n",
    "    pre_channel = num_channel[i]\n",
    "pre_feature = num_features\n",
    "for  i in range(len(num_layers)):\n",
    "    p += (pre_feature+1)*num_layers[i]\n",
    "    pre_feature = num_layers[i]\n",
    "\n",
    "const_q = (u_0+1)*np.log(p) + 0.5*np.log(rho_0/rho_1)\n",
    "scale = 1.0*sple_size\n",
    "p,const_q,scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify(model, param, param_struct):\n",
    "    for i in range(conv_layer):\n",
    "        weights, biases = param[conv_param_name[2*i]], param[conv_param_name[2*i+1]]\n",
    "        weights = weights*param_struct[conv_param_name[2*i]]\n",
    "        biases = biases*param_struct[conv_param_name[2*i+1]]\n",
    "        model.layers[conv_layer_idx[i]].set_weights([weights, biases])\n",
    "    for i in range(full_layer):\n",
    "        weights, biases = param[full_param_name[2*i]], param[full_param_name[2*i+1]]\n",
    "        weights = weights*param_struct[full_param_name[2*i]]\n",
    "        biases = biases*param_struct[full_param_name[2*i+1]]\n",
    "        model.layers[i+conv_layer+pool_layer+1].set_weights([weights, biases])#plus 1 is the flatten layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def grad_loss(model, X, y):\n",
    "    loss_fun = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    with tf.GradientTape() as tape:  \n",
    "        y_ = model(X)\n",
    "        lp_value = loss_fun(y, y_)\n",
    "    return tape.gradient(lp_value, model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv_param_name = ['ConvW1','Convb1','ConvW2','Convb2','ConvW3',\n",
    "#                   'Convb3','ConvW4','Convb4','ConvW5','Convb5']\n",
    "#full_param_name = ['W1','b1','W2','b2','W3','b3']\n",
    "\n",
    "conv_param_name = ['ConvW1','Convb1','ConvW2','Convb2']\n",
    "full_param_name = ['W1','b1','W2','b2','W3','b3']\n",
    "\n",
    "param,param_struct,param_struct_tmp = {},{},{}\n",
    "for i in range(conv_layer):\n",
    "    if i == 0:\n",
    "        param_struct[conv_param_name[2*i]] = np.ones(shape = (kernel_size[i],kernel_size[i],1,num_channel[i]))\n",
    "        param_struct_tmp[conv_param_name[2*i]] = np.ones(shape = (kernel_size[i],kernel_size[i],1,num_channel[i]))\n",
    "    else:\n",
    "        param_struct[conv_param_name[2*i]] = np.ones(shape = (kernel_size[i],kernel_size[i],num_channel[i-1],num_channel[i]))\n",
    "        param_struct_tmp[conv_param_name[2*i]] = np.ones(shape = (kernel_size[i],kernel_size[i],num_channel[i-1],num_channel[i]))\n",
    "    param[conv_param_name[2*i]] = model.trainable_variables[2*i].numpy()\n",
    "    param[conv_param_name[2*i+1]] = model.trainable_variables[2*i+1].numpy()\n",
    "    param_struct[conv_param_name[2*i+1]] = np.ones(shape = num_channel[i])\n",
    "    param_struct_tmp[conv_param_name[2*i+1]] = np.ones(shape = num_channel[i])\n",
    "\n",
    "for i in range(full_layer):\n",
    "    if i == 0:\n",
    "        param_struct[full_param_name[2*i]] = np.ones(shape = (flatten,num_layers[i]))\n",
    "        param_struct_tmp[full_param_name[2*i]] = np.ones(shape = (flatten,num_layers[i]))\n",
    "    else:\n",
    "        param_struct[full_param_name[2*i]] = np.ones(shape = (num_layers[i-1],num_layers[i]))\n",
    "        param_struct_tmp[full_param_name[2*i]] = np.ones(shape = (num_layers[i-1],num_layers[i]))\n",
    "    param[full_param_name[2*i]] = model.trainable_variables[2*(conv_layer+i)].numpy() \n",
    "    param[full_param_name[2*i+1]] = model.trainable_variables[2*(conv_layer+i)+1].numpy() \n",
    "    param_struct[full_param_name[2*i+1]] = np.ones(shape = num_layers[i])\n",
    "    param_struct_tmp[full_param_name[2*i+1]] = np.ones(shape = num_layers[i])\n",
    "        \n",
    "coordset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse_size = np.array([1000,50,10000,50,10000,50,10000,50,5000,50,10000,50,10000,50,500,10])\n",
    "#conv_layer_idx = np.array([0,2,4,5,6])\n",
    "sparse_size = np.array([10,1,10,1,300,10,200,10,10,0])\n",
    "conv_layer_idx = np.array([0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSparsityStructure(param_struct, param_struct_tmp, coordset):\n",
    "    for i in range(conv_layer):\n",
    "        if i == 0:\n",
    "            sel_W = np.random.choice(a = kernel_size[i]*kernel_size[i]*num_channel[i],size = sparse_size[2*i],replace = False)\n",
    "            sel_b = np.random.choice(a = num_channel[i],size = sparse_size[2*i+1],replace = False)\n",
    "        else:\n",
    "            sel_W = np.random.choice(a = kernel_size[i]*kernel_size[i]*num_channel[i-1]*num_channel[i],size = sparse_size[2*i],replace = False)\n",
    "            sel_b = np.random.choice(a = num_channel[i],size = sparse_size[2*i+1],replace = False)\n",
    "        param_struct_tmp[conv_param_name[2*i]][:] = param_struct[conv_param_name[2*i]][:]\n",
    "        param_struct_tmp[conv_param_name[2*i+1]][:] = param_struct[conv_param_name[2*i+1]][:]\n",
    "        np.reshape(param_struct_tmp[conv_param_name[2*i]],-1)[sel_W] = 0\n",
    "        param_struct_tmp[conv_param_name[2*i+1]][sel_b] = 0\n",
    "        coordset[conv_param_name[2*i]] = sel_W\n",
    "        coordset[conv_param_name[2*i+1]] =sel_b\n",
    "        \n",
    "    for i in range(full_layer):\n",
    "        if i == 0:\n",
    "            sel_W = np.random.choice(a=num_features * num_layers[i],size = sparse_size[(conv_layer+i)*2], replace =False)\n",
    "            sel_b = np.random.choice(a=num_layers[i],size = sparse_size[(conv_layer+i)*2+1], replace =False)\n",
    "        else:\n",
    "            sel_W = np.random.choice(a = num_layers[i-1] * num_layers[i],size = sparse_size[(conv_layer+i)*2],replace = False)\n",
    "            sel_b = np.random.choice(a = num_layers[i],size = sparse_size[(conv_layer+i)*2+1],replace = False)\n",
    "        param_struct_tmp[full_param_name[2*i]][:] = param_struct[full_param_name[2*i]][:]\n",
    "        param_struct_tmp[full_param_name[2*i+1]][:] = param_struct[full_param_name[2*i+1]][:]\n",
    "        np.reshape(param_struct_tmp[full_param_name[2*i]],-1)[sel_W] = 0\n",
    "        param_struct_tmp[full_param_name[2*i+1]][sel_b] = 0\n",
    "        coordset[full_param_name[2*i]] = sel_W\n",
    "        coordset[full_param_name[2*i+1]] =sel_b \n",
    "    return param_struct_tmp, coordset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateStructure(model,param,param_struct,param_struct_tmp,coordset,minibatch_X,minibatch_y):\n",
    "    model = sparsify(model, param, param_struct_tmp)\n",
    "    gradients = grad_loss(model, minibatch_X, minibatch_y)\n",
    "    #update structure of delta\n",
    "    for i in range(conv_layer):\n",
    "        weights, biases = param[conv_param_name[2*i]], param[conv_param_name[2*i+1]]\n",
    "        weight_index = coordset[conv_param_name[2*i]]\n",
    "        bias_index = coordset[conv_param_name[2*i+1]]\n",
    "        #update W\n",
    "        grad = gradients[2*i].numpy()\n",
    "        grad = np.reshape(grad,-1)[weight_index]\n",
    "        vec_temp = scale*grad\n",
    "        weights = np.reshape(weights,-1)[weight_index]\n",
    "        zz1 = -weights* vec_temp\n",
    "        zz2 = 0.5*(rho_1-rho_0)*(weights**2)\n",
    "        zz = const_q - zz1 + zz2 - 0.5*(zz1**2)\n",
    "        prob = expit(-zz)\n",
    "        np.reshape(param_struct[conv_param_name[2*i]],-1)[weight_index] = np.random.binomial(1,prob)\n",
    "        #update b\n",
    "        grad = gradients[2*i+1].numpy()\n",
    "        biases = biases[bias_index]\n",
    "        vec_temp = scale*grad[bias_index]\n",
    "        zz1 = -biases* vec_temp\n",
    "        zz2 = 0.5*(rho_1-rho_0)*(biases**2)\n",
    "        zz = const_q - zz1 + zz2 - 0.5*(zz1**2)\n",
    "        prob = expit(-zz)\n",
    "        param_struct[conv_param_name[2*i+1]][bias_index] = np.random.binomial(1,prob)\n",
    "        \n",
    "    for i in range(full_layer):\n",
    "        weights, biases = param[full_param_name[2*i]], param[full_param_name[2*i+1]]\n",
    "        weight_index = coordset[full_param_name[2*i]]\n",
    "        bias_index = coordset[full_param_name[2*i+1]]\n",
    "        #update W\n",
    "        grad = gradients[2*(conv_layer+i)].numpy()\n",
    "        grad = np.reshape(grad,-1)[weight_index]\n",
    "        vec_temp = scale*grad\n",
    "        weights = np.reshape(weights,-1)[weight_index]\n",
    "        zz1 = -weights* vec_temp\n",
    "        zz2 = 0.5*(rho_1-rho_0)*(weights**2)\n",
    "        zz = const_q - zz1 + zz2 - 0.5*(zz1**2)\n",
    "        prob = expit(-zz)\n",
    "        np.reshape(param_struct[full_param_name[2*i]],-1)[weight_index] = np.random.binomial(1,prob)\n",
    "        #update b\n",
    "        grad = gradients[2*(conv_layer+i)+1].numpy()\n",
    "        vec_temp = scale*grad[bias_index]\n",
    "        biases = biases[bias_index]\n",
    "        zz1 = -biases* vec_temp\n",
    "        zz2 = 0.5*(rho_1-rho_0)*(biases**2)\n",
    "        zz = const_q - zz1 + zz2 - 0.5*(zz1**2)\n",
    "        prob = expit(-zz)\n",
    "        param_struct[full_param_name[2*i+1]][bias_index] = np.random.binomial(1,prob)\n",
    "    return param,param_struct,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParam(model, param,param_struct,minibatch_X,minibatch_y):\n",
    "    model = sparsify(model, param, param_struct)\n",
    "    gradients = grad_loss(model, minibatch_X, minibatch_y)\n",
    "    #Updatae conv layer:\n",
    "    for i in range(conv_layer):\n",
    "        L = np.sum(param_struct[conv_param_name[2*i]]==0)\n",
    "        if L > 0:\n",
    "            param[conv_param_name[2*i]][param_struct[conv_param_name[2*i]]==0] = np.random.normal(0.0, np.sqrt(1/rho_0), L)\n",
    "        L = np.sum(param_struct[conv_param_name[2*i+1]]==0)\n",
    "        if L > 0:\n",
    "            param[conv_param_name[2*i+1]][param_struct[conv_param_name[2*i+1]]==0] = np.random.normal(0.0, np.sqrt(1/rho_0), L)    \n",
    "        L = np.sum(param_struct[conv_param_name[2*i]]==1)\n",
    "        if L > 0:\n",
    "            sub_grad = gradients[2*i][param_struct[conv_param_name[2*i]]==1].numpy()\n",
    "            sub_grad = -scale*sub_grad - rho_1*param[conv_param_name[2*i]][param_struct[conv_param_name[2*i]]==1]\n",
    "            sub_weights = param[conv_param_name[2*i]][param_struct[conv_param_name[2*i]]==1] + 0.5*gamma*sub_grad + np.sqrt(gamma)*np.random.normal(0.0, 1.0,L)\n",
    "            param[conv_param_name[2*i]][param_struct[conv_param_name[2*i]]==1] = sub_weights\n",
    "        L = np.sum(param_struct[conv_param_name[2*i+1]]==1)\n",
    "        if L > 0:\n",
    "            sub_grad = gradients[2*i+1][param_struct[conv_param_name[2*i+1]]==1].numpy()\n",
    "            sub_grad = -scale*sub_grad - rho_1*param[conv_param_name[2*i+1]][param_struct[conv_param_name[2*i+1]]==1]\n",
    "            sub_bias = param[conv_param_name[2*i+1]][param_struct[conv_param_name[2*i+1]]==1] + 0.5*gamma*sub_grad + np.sqrt(gamma)*np.random.normal(0.0, 1.0,L)\n",
    "            param[conv_param_name[2*i+1]][param_struct[conv_param_name[2*i+1]]==1] = sub_bias\n",
    "            \n",
    "    for i in range(full_layer):\n",
    "        L = np.sum(param_struct[full_param_name[2*i]]==0)\n",
    "        if L > 0:\n",
    "            param[full_param_name[2*i]][param_struct[full_param_name[2*i]]==0] = np.random.normal(0.0, np.sqrt(1/rho_0), L)\n",
    "        L = np.sum(param_struct[full_param_name[2*i+1]]==0)\n",
    "        if L > 0:\n",
    "            param[full_param_name[2*i+1]][param_struct[full_param_name[2*i+1]]==0] = np.random.normal(0.0, np.sqrt(1/rho_0), L)    \n",
    "        L = np.sum(param_struct[full_param_name[2*i]]==1)\n",
    "        if L > 0:\n",
    "            sub_grad = gradients[2*(conv_layer+i)][param_struct[full_param_name[2*i]]==1].numpy()\n",
    "            sub_grad = -scale*sub_grad - rho_1*param[full_param_name[2*i]][param_struct[full_param_name[2*i]]==1]\n",
    "            sub_weights = param[full_param_name[2*i]][param_struct[full_param_name[2*i]]==1] + 0.5*gamma*sub_grad + np.sqrt(gamma)*np.random.normal(0.0, 1.0,L)\n",
    "            param[full_param_name[2*i]][param_struct[full_param_name[2*i]]==1] = sub_weights\n",
    "        L = np.sum(param_struct[full_param_name[2*i+1]]==1)\n",
    "        if L > 0:\n",
    "            sub_grad = gradients[2*(conv_layer+i)+1][param_struct[full_param_name[2*i+1]]==1].numpy()\n",
    "            sub_grad = -scale*sub_grad - rho_1*param[full_param_name[2*i+1]][param_struct[full_param_name[2*i+1]]==1]\n",
    "            sub_bias = param[full_param_name[2*i+1]][param_struct[full_param_name[2*i+1]]==1] + 0.5*gamma*sub_grad + np.sqrt(gamma)*np.random.normal(0.0, 1.0,L)\n",
    "            param[full_param_name[2*i+1]][param_struct[full_param_name[2*i+1]]==1] = sub_bias\n",
    "    return param, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 [0.3413     0.91607567]\n",
      "99 [0.3968     0.84232379]\n",
      "149 [0.6073     0.77802444]\n",
      "199 [0.6812     0.72259166]\n"
     ]
    }
   ],
   "source": [
    "Output = np.zeros(shape=[numIter,2])\n",
    "for kk in range(numIter):\n",
    "    #Draw a mini batch\n",
    "    rand_index = np.random.choice(sple_size, minibatch_size)\n",
    "    minibatch_X, minibatch_y = tf.gather(X_train,rand_index), tf.gather(y_train,rand_index)\n",
    "    \n",
    "    # Update sparsity structure\n",
    "    param_struct_tmp, coordset = updateSparsityStructure(param_struct, param_struct_tmp, coordset)\n",
    "    \n",
    "    #update structure of delta\n",
    "    param, param_struct,model = updateStructure(model,param,param_struct,param_struct_tmp,coordset,minibatch_X,minibatch_y)\n",
    "\n",
    "    #update parameter\n",
    "    param, model = updateParam(model,param,param_struct,minibatch_X,minibatch_y)\n",
    "    \n",
    "    if (kk+1)%50== 0:\n",
    "        model = sparsify(model, param, param_struct)\n",
    "        y_pred = model(X_test)\n",
    "        pred = np.argmax(y_pred, axis=-1)\n",
    "        Output[kk,0] = np.sum(pred == y_test) / len(y_test)\n",
    "        nnz = 0\n",
    "        for i in range(conv_layer):\n",
    "            nnz += np.count_nonzero(param_struct[conv_param_name[2*i]]) + np.count_nonzero(param_struct[conv_param_name[2*i+1]]) \n",
    "        for i in range(full_layer):\n",
    "            nnz += np.count_nonzero(param_struct[full_param_name[2*i]]) + np.count_nonzero(param_struct[full_param_name[2*i+1]]) \n",
    "        Output[kk,1] = nnz/p\n",
    "        print(kk,Output[kk,:])\n",
    "    else:\n",
    "        Output[kk,0] = 0\n",
    "    \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
